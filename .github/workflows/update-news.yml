name: Update AI News Every 2 Hours

on:
  schedule:
    # Run every 2 hours
    - cron: '0 */2 * * *'
  
  # Allow manual trigger
  workflow_dispatch:

jobs:
  update-news:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 feedparser
      
      - name: Fetch latest AI news
        run: |
          python scripts/fetch_news.py
      
      - name: Update HTML file
        run: |
          python scripts/update_html.py
      
      - name: Commit and push if changed
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add index.html
          git diff --quiet && git diff --staged --quiet || (git commit -m "ğŸ¤– Auto-update: Latest AI news $(date)" && git push)
```

4. æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç‚¹å‡» **"Commit new file"**

---

### ç¬¬2æ­¥ï¼šä¸Šä¼ ç¬¬ä¸€ä¸ªPythonè„šæœ¬

1. å†æ¬¡ç‚¹å‡» **"Add file"** â†’ **"Create new file"**

2. æ–‡ä»¶åè¾“å…¥ï¼š
```
   scripts/fetch_news.py#!/usr/bin/env python3
"""
AI News Fetcher - Fetch latest AI news every 2 hours
"""

import json
import requests
from datetime import datetime
import feedparser
import time

NEWS_SOURCES = {
    'rss_feeds': [
        'https://techcrunch.com/tag/artificial-intelligence/feed/',
        'https://www.technologyreview.com/feed/',
        'https://www.theverge.com/ai-artificial-intelligence/rss/index.xml',
        'https://www.wired.com/feed/tag/ai/latest/rss',
    ],
    'reddit': [
        'https://www.reddit.com/r/artificial/.json',
        'https://www.reddit.com/r/MachineLearning/.json',
    ]
}

def fetch_rss_news():
    """Fetch news from RSS feeds"""
    news_items = []
    
    for feed_url in NEWS_SOURCES['rss_feeds']:
        try:
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:5]:
                news_items.append({
                    'platform': 'twitter',
                    'title': entry.get('title', 'No title'),
                    'author': entry.get('author', feed.feed.get('title', 'Unknown')),
                    'engagement': 0,
                    'engagement_type': 'likes',
                    'date': datetime.now().strftime('%Y-%m-%d'),
                    'url': entry.get('link', '#'),
                    'thumbnail': None
                })
        except Exception as e:
            print(f"Error fetching RSS {feed_url}: {e}")
    
    return news_items

def fetch_reddit_news():
    """Fetch AI discussions from Reddit"""
    news_items = []
    headers = {'User-Agent': 'AI News Bot 1.0'}
    
    for subreddit_url in NEWS_SOURCES['reddit']:
        try:
            response = requests.get(subreddit_url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                for post in data['data']['children'][:10]:
                    post_data = post['data']
                    news_items.append({
                        'platform': 'twitter',
                        'title': post_data.get('title', 'No title'),
                        'author': f"r/{post_data.get('subreddit', 'unknown')}",
                        'engagement': post_data.get('score', 0),
                        'engagement_type': 'likes',
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'url': f"https://reddit.com{post_data.get('permalink', '')}",
                        'thumbnail': None
                    })
            time.sleep(1)
        except Exception as e:
            print(f"Error fetching Reddit {subreddit_url}: {e}")
    
    return news_items

def main():
    print("Fetching AI news...")
    
    all_news = []
    
    # Fetch from RSS
    rss_news = fetch_rss_news()
    all_news.extend(rss_news)
    print(f"Got {len(rss_news)} items from RSS")
    
    # Fetch from Reddit
    reddit_news = fetch_reddit_news()
    all_news.extend(reddit_news)
    print(f"Got {len(reddit_news)} items from Reddit")
    
    # Remove duplicates
    seen_titles = set()
    unique_news = []
    for item in all_news:
        if item['title'] not in seen_titles:
            seen_titles.add(item['title'])
            unique_news.append(item)
    
    # Sort by engagement
    unique_news.sort(key=lambda x: x['engagement'], reverse=True)
    
    # Save to JSON
    with open('news_data.json', 'w', encoding='utf-8') as f:
        json.dump(unique_news, f, ensure_ascii=False, indent=2)
    
    print(f"Saved {len(unique_news)} unique news items")

if __name__ == '__main__':
    main()
```

4. ç‚¹å‡» **"Commit new file"**

---

### ç¬¬3æ­¥ï¼šä¸Šä¼ ç¬¬äºŒä¸ªPythonè„šæœ¬

1. å†æ¬¡ç‚¹å‡» **"Add file"** â†’ **"Create new file"**

2. æ–‡ä»¶åè¾“å…¥ï¼š
```
   scripts/update_html.py#!/usr/bin/env python3
"""
Update HTML with fetched news data
"""

import json
from datetime import datetime

def update_html():
    """Read news data and update HTML"""
    
    # Read news data
    with open('news_data.json', 'r', encoding='utf-8') as f:
        news_data = json.load(f)
    
    # Read HTML template
    with open('index.html', 'r', encoding='utf-8') as f:
        html_content = f.read()
    
    # Find and replace data
    start_marker = "const episodes = ["
    end_marker = "];"
    
    start_idx = html_content.find(start_marker)
    end_idx = html_content.find(end_marker, start_idx)
    
    if start_idx != -1 and end_idx != -1:
        # Generate new data string
        new_data = json.dumps(news_data, ensure_ascii=False, indent=2)
        
        # Replace
        new_html = (
            html_content[:start_idx + len(start_marker)] +
            "\n" + new_data + "\n        " +
            html_content[end_idx:]
        )
        
        # Update timestamp
        update_time = datetime.now().strftime('%B %d, %Y')
        new_html = new_html.replace(
            'Updated January 29, 2026',
            f'Updated {update_time}'
        )
        
        # Write file
        with open('index.html', 'w', encoding='utf-8') as f:
            f.write(new_html)
        
        print(f"HTML updated with {len(news_data)} news items")
    else:
        print("Could not find data placeholder in HTML")

if __name__ == '__main__':
    update_html()
```

4. ç‚¹å‡» **"Commit new file"**

---

### ç¬¬4æ­¥ï¼šæ›¿æ¢ index.html

1. åœ¨ä»“åº“ä¸»é¡µï¼Œç‚¹å‡» **`index.html`** æ–‡ä»¶

2. ç‚¹å‡»å³ä¸Šè§’çš„ **é“…ç¬”å›¾æ ‡**ï¼ˆç¼–è¾‘ï¼‰

3. **åˆ é™¤æ‰€æœ‰æ—§å†…å®¹**

4. ä»æˆ‘ä¹‹å‰ç»™æ‚¨çš„æ–‡ä»¶ä¸­ï¼Œæ‰“å¼€æ–°çš„ `index.html`ï¼Œå¤åˆ¶å…¨éƒ¨å†…å®¹ç²˜è´´è¿›å»

5. æ»šåŠ¨åˆ°åº•éƒ¨ï¼Œç‚¹å‡» **"Commit changes"**

---

### ç¬¬5æ­¥ï¼šé…ç½®æƒé™ âš ï¸ é‡è¦ï¼

1. ç‚¹å‡»ä»“åº“é¡¶éƒ¨çš„ **Settings**

2. å·¦ä¾§èœå•æ‰¾åˆ° **Actions** â†’ ç‚¹å‡» **General**

3. å¾€ä¸‹æ»šåŠ¨åˆ° **"Workflow permissions"**

4. é€‰æ‹© âœ… **"Read and write permissions"**

5. ç‚¹å‡» **Save**

---

### ç¬¬6æ­¥ï¼šæµ‹è¯•è¿è¡Œ

1. ç‚¹å‡»ä»“åº“é¡¶éƒ¨çš„ **Actions** æ ‡ç­¾

2. å·¦ä¾§åº”è¯¥çœ‹åˆ° **"Update AI News Every 2 Hours"**

3. ç‚¹å‡»å³ä¾§çš„ **"Run workflow"** æŒ‰é’®

4. å†ç‚¹å‡»ç»¿è‰²çš„ **"Run workflow"**

5. ç­‰å¾…2-3åˆ†é’Ÿï¼Œçœ‹åˆ°ç»¿è‰²âœ…å°±æˆåŠŸäº†ï¼

---

## âœ… å®Œæˆåæ£€æŸ¥

æœ€ç»ˆæ‚¨çš„ä»“åº“åº”è¯¥æœ‰è¿™äº›æ–‡ä»¶ï¼š
```
âœ… .github/workflows/update-news.yml
âœ… scripts/fetch_news.py
âœ… scripts/update_html.py
âœ… index.html

